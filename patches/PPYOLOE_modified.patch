diff --git a/configs/ppyoloe/ppyoloe_m_custom_seal.py b/configs/ppyoloe/ppyoloe_m_custom_seal.py
new file mode 100644
index 0000000..28042ff
--- /dev/null
+++ b/configs/ppyoloe/ppyoloe_m_custom_seal.py
@@ -0,0 +1,96 @@
+_base_ = '/home/computador/Desktop/models_research/loss/mmyolo/configs/ppyoloe/ppyoloe_plus_s_fast_8xb8-80e_coco.py'
+
+
+# Dataset root
+data_root = '/home/computador/Desktop/models_research/mmyolo/Dataset/'
+class_name = ('Seal', 'Tag_White', 'Tag_Yellow')
+num_classes = len(class_name)
+metainfo = dict(classes=class_name)
+
+
+# Model config
+model = dict(
+    bbox_head=dict(
+        head_module=dict(num_classes=num_classes)
+    ),
+    neck=dict(
+        type='PPYOLOECSPPAFPN',
+        use_spp=False,
+        fusion_mode='asff',  #  by @manikanta
+    ),
+    train_cfg=dict(
+        initial_assigner=dict(num_classes=num_classes),
+        assigner=dict(num_classes=num_classes)
+    )
+)
+
+
+# ðŸ”§ Dataloaders
+train_dataloader = dict(
+    batch_size=6,
+    num_workers=4,
+    dataset=dict(
+        type='YOLOv5CocoDataset',
+        data_root=data_root,
+        metainfo=metainfo,
+        ann_file='train/_annotations.coco.json',
+        data_prefix=dict(img='train/'),
+
+    )
+)
+
+val_dataloader = dict(
+    batch_size=6,
+    num_workers=4,
+    dataset=dict(
+        type='YOLOv5CocoDataset',
+        data_root=data_root,
+        metainfo=metainfo,
+        ann_file='valid/_annotations.coco.json',
+        data_prefix=dict(img='valid/')
+    )
+)
+
+
+test_dataloader = dict(
+    batch_size=6,
+    num_workers=4,
+    dataset=dict(
+        type='YOLOv5CocoDataset',
+        data_root=data_root,
+        metainfo=metainfo,
+        ann_file='test/_annotations.coco.json',   
+        data_prefix=dict(img='test/')       
+    )
+)
+
+# Evaluators
+
+val_evaluator = dict(ann_file=data_root + 'valid/_annotations.coco.json')
+test_evaluator = dict(ann_file=data_root + 'test/_annotations.coco.json')
+
+# Training schedule
+max_epochs = 100
+train_cfg = dict(max_epochs=max_epochs, val_interval=10)
+
+# Logger & checkpoints
+default_hooks = dict(
+    checkpoint=dict(interval=10, max_keep_ckpts=2, save_best='auto'),
+    logger=dict(type='LoggerHook', interval=5),
+    param_scheduler=dict(
+        warmup_min_iter=10,
+        warmup_epochs=3,
+        total_epochs=int(max_epochs * 1.2)
+    )
+)
+
+# Load pretrained weights (still use PP-YOLOE-S pretrained)
+load_from = 'https://download.openmmlab.com/mmyolo/v0/ppyoloe/ppyoloe_plus_s_fast_8xb8-80e_coco/ppyoloe_plus_s_fast_8xb8-80e_coco_20230101_154052-9fee7619.pth'
+
+visualizer = dict(
+    vis_backends=[dict(type='LocalVisBackend')]
+)
+
+optim_wrapper = dict(
+    optimizer=dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=5e-4)
+)
diff --git a/mani-test/metrics.ipynb b/mani-test/metrics.ipynb
new file mode 100644
index 0000000..eb0366e
--- /dev/null
+++ b/mani-test/metrics.ipynb
@@ -0,0 +1,457 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "id": "13576f89",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/computador/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
+      "  from .autonotebook import tqdm as notebook_tqdm\n",
+      "/home/computador/.local/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
+      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
+      "/home/computador/.local/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
+      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
+     ]
+    }
+   ],
+   "source": [
+    "import os\n",
+    "import json\n",
+    "from glob import glob\n",
+    "from mmengine import Config\n",
+    "from mmdet.apis import init_detector, inference_detector"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "id": "b4c931fa",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "test_img_dir = '/home/computador/Desktop/Universal_test_file/test'   # <-- put your test images here\n",
+    "gt_ann_file    = '/home/computador/Desktop/Universal_test_file/test/annotations_fixed.json'  # <-- your GT JSON\n",
+    "img_dir = test_img_dir"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "id": "664205f0",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# ---------------------------------------------------------\n",
+    "config_file = '/home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/work_dirs/ppyoloe_m_custom_seal/ppyoloe_m_custom_seal.py'\n",
+    "checkpoint_file = '/home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/work_dirs/ppyoloe_m_custom_seal/epoch_100.pth'\n",
+    "\n",
+    "out_json = '/home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/pred_annotations.json'\n",
+    "\n",
+    "# ---------------------------------------------------------"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "id": "3422f0c5",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/computador/.local/lib/python3.8/site-packages/mmcv/cnn/bricks/hsigmoid.py:35: UserWarning: In MMCV v1.4.4, we modified the default value of args to align with PyTorch official. Previous Implementation: Hsigmoid(x) = min(max((x + 1) / 2, 0), 1). Current Implementation: Hsigmoid(x) = min(max((x + 3) / 6, 0), 1).\n",
+      "  warnings.warn(\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Loads checkpoint by local backend from path: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/work_dirs/ppyoloe_m_custom_seal/epoch_100.pth\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/computador/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
+      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Prediction annotations saved to: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/pred_annotations.json\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Load ground-truth annotations\n",
+    "with open(gt_ann_file) as f:\n",
+    "    gt = json.load(f)\n",
+    "\n",
+    "# Map filename â†’ COCO image_id\n",
+    "fname_to_id = {os.path.basename(im['file_name']): im['id'] for im in gt['images']}\n",
+    "\n",
+    "# Model label â†’ GT category_id mapping\n",
+    "# Assuming your model outputs labels: 0 â†’ Seal, 1 â†’ Tag_White, 2 â†’ Tag_Yellow\n",
+    "model_to_gt = {0: 0, 1: 1, 2: 2}\n",
+    "\n",
+    "cfg   = Config.fromfile(config_file)\n",
+    "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
+    "\n",
+    "# Collect test images\n",
+    "img_paths = sorted(\n",
+    "    glob(os.path.join(test_img_dir, '*.jpg')) +\n",
+    "    glob(os.path.join(test_img_dir, '*.png')) +\n",
+    "    glob(os.path.join(test_img_dir, '*.jpeg'))\n",
+    ")\n",
+    "\n",
+    "results, ann_id = [], 1\n",
+    "score_thr = 0.5  # minimum confidence to keep a prediction\n",
+    "\n",
+    "for img_path in img_paths:\n",
+    "    fname    = os.path.basename(img_path)\n",
+    "    image_id = fname_to_id[fname]  # use GT image_id\n",
+    "\n",
+    "    det  = inference_detector(model, img_path)\n",
+    "    inst = det.pred_instances\n",
+    "\n",
+    "    bboxes = inst.bboxes.cpu().numpy()\n",
+    "    scores = inst.scores.cpu().numpy()\n",
+    "    labels = inst.labels.cpu().numpy()\n",
+    "\n",
+    "    for box, score, label in zip(bboxes, scores, labels):\n",
+    "        if score < score_thr:\n",
+    "            continue  # skip low-confidence boxes\n",
+    "\n",
+    "        # map model label to GT category_id\n",
+    "        category_id = model_to_gt[int(label)]\n",
+    "\n",
+    "        x1, y1, x2, y2 = box.tolist()\n",
+    "        results.append({\n",
+    "            \"id\": ann_id,\n",
+    "            \"image_id\": image_id,\n",
+    "            \"file_name\": fname,\n",
+    "            \"category_id\": category_id,\n",
+    "            \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
+    "            \"score\": float(score)\n",
+    "        })\n",
+    "        ann_id += 1\n",
+    "\n",
+    "# Save predictions in COCO JSON format\n",
+    "with open(out_json, 'w') as f:\n",
+    "    json.dump(results, f)\n",
+    "\n",
+    "print(f\"Prediction annotations saved to: {out_json}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 14,
+   "id": "b8b56e90",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "{'mAP@[0.5:0.95]': 0.15924802641685037, 'AP50': 0.4266141624226978, 'AP75': 0.07263256069355031}\n"
+     ]
+    }
+   ],
+   "source": [
+    "# No COCO API\n",
+    "import json\n",
+    "from collections import defaultdict\n",
+    "import numpy as np\n",
+    "\n",
+    "def iou(box1, box2):\n",
+    "    \"\"\"Compute IoU between two boxes in [x,y,w,h] format.\"\"\"\n",
+    "    x1, y1, w1, h1 = box1\n",
+    "    x2, y2, w2, h2 = box2\n",
+    "    xa = max(x1, x2)\n",
+    "    ya = max(y1, y2)\n",
+    "    xb = min(x1 + w1, x2 + w2)\n",
+    "    yb = min(y1 + h1, y2 + h2)\n",
+    "    inter = max(0, xb - xa) * max(0, yb - ya)\n",
+    "    union = w1 * h1 + w2 * h2 - inter\n",
+    "    return inter / union if union > 0 else 0.0\n",
+    "\n",
+    "def load_boxes(path, relevant_cat_ids=None, is_pred=False):\n",
+    "    with open(path) as f:\n",
+    "        data = json.load(f)\n",
+    "    boxes = defaultdict(list)\n",
+    "    anns = data['annotations'] if not is_pred else data\n",
+    "    for ann in anns:\n",
+    "        if relevant_cat_ids and ann['category_id'] not in relevant_cat_ids:\n",
+    "            continue\n",
+    "        img_id = ann['image_id']\n",
+    "        boxes[img_id].append({\n",
+    "            'bbox': ann['bbox'],\n",
+    "            'score': ann.get('score', 1.0),  # preds should have score\n",
+    "            'cat': ann['category_id']\n",
+    "        })\n",
+    "    return boxes\n",
+    "\n",
+    "def average_precision(rec, prec):\n",
+    "    # 11-point interpolation (Pascal) or COCOâ€™s continuous method\n",
+    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
+    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
+    "    for i in range(mpre.size - 1, 0, -1):\n",
+    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
+    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
+    "    return np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n",
+    "\n",
+    "def evaluate(gt_boxes, pred_boxes, iou_thr=0.5, cat_ids=None):\n",
+    "    aps = []\n",
+    "    for cat in cat_ids:\n",
+    "        all_scores = []\n",
+    "        all_matches = []\n",
+    "        npos = 0\n",
+    "        for img_id in gt_boxes.keys():\n",
+    "            gt_for_img = [g for g in gt_boxes[img_id] if g['cat']==cat]\n",
+    "            pred_for_img = [p for p in pred_boxes[img_id] if p['cat']==cat]\n",
+    "            npos += len(gt_for_img)\n",
+    "            gt_matched = [False]*len(gt_for_img)\n",
+    "            # sort preds by score\n",
+    "            pred_for_img.sort(key=lambda x: x['score'], reverse=True)\n",
+    "            for p in pred_for_img:\n",
+    "                best_iou = 0\n",
+    "                best_j = -1\n",
+    "                for j,g in enumerate(gt_for_img):\n",
+    "                    i = iou(p['bbox'], g['bbox'])\n",
+    "                    if i>best_iou:\n",
+    "                        best_iou, best_j = i, j\n",
+    "                if best_iou >= iou_thr and not gt_matched[best_j]:\n",
+    "                    all_matches.append(1)\n",
+    "                    gt_matched[best_j] = True\n",
+    "                else:\n",
+    "                    all_matches.append(0)\n",
+    "                all_scores.append(p['score'])\n",
+    "        if npos==0:\n",
+    "            continue\n",
+    "        # sort all by score\n",
+    "        idxs = np.argsort(-np.array(all_scores))\n",
+    "        tp = np.cumsum(np.array(all_matches)[idxs])\n",
+    "        fp = np.cumsum(1 - np.array(all_matches)[idxs])\n",
+    "        rec = tp / npos\n",
+    "        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
+    "        aps.append(average_precision(rec, prec))\n",
+    "    return np.mean(aps) if aps else 0.0\n",
+    "\n",
+    "# === Usage ===\n",
+    "gt_path   = gt_ann_file\n",
+    "pred_path = out_json\n",
+    "relevant_cat_ids = [1,2,3]   # choose the ids you care about\n",
+    "\n",
+    "gt_boxes   = load_boxes(gt_path, relevant_cat_ids)\n",
+    "pred_boxes = load_boxes(pred_path, relevant_cat_ids, is_pred=True)\n",
+    "\n",
+    "mAP_50  = evaluate(gt_boxes, pred_boxes, iou_thr=0.5, cat_ids=relevant_cat_ids)\n",
+    "mAP_75  = evaluate(gt_boxes, pred_boxes, iou_thr=0.75, cat_ids=relevant_cat_ids)\n",
+    "\n",
+    "# For COCO-style mAP@[0.5:0.95]\n",
+    "thr_range = np.arange(0.5, 1.0, 0.05)\n",
+    "mAP = np.mean([evaluate(gt_boxes, pred_boxes, t, relevant_cat_ids) for t in thr_range])\n",
+    "\n",
+    "print({\n",
+    "    \"mAP@[0.5:0.95]\": mAP,\n",
+    "    \"AP50\": mAP_50,\n",
+    "    \"AP75\": mAP_75\n",
+    "})\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "id": "ccf00ba8",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "pred_json = out_json"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 16,
+   "id": "444f21f2",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "loading annotations into memory...\n",
+      "Done (t=0.00s)\n",
+      "creating index...\n",
+      "index created!\n",
+      "Loading and preparing results...\n",
+      "DONE (t=0.00s)\n",
+      "creating index...\n",
+      "index created!\n",
+      "Running per image evaluation...\n",
+      "Evaluate annotation type *bbox*\n",
+      "DONE (t=0.17s).\n",
+      "Accumulating evaluation results...\n",
+      "DONE (t=0.04s).\n",
+      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.387\n",
+      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.608\n",
+      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.358\n",
+      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.107\n",
+      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
+      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.878\n",
+      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.432\n",
+      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
+      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.432\n",
+      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.141\n",
+      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
+      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.898\n"
+     ]
+    }
+   ],
+   "source": [
+    "from pycocotools.coco import COCO\n",
+    "from pycocotools.cocoeval import COCOeval\n",
+    "\n",
+    "coco_gt = COCO(gt_ann_file)\n",
+    "coco_dt = coco_gt.loadRes(pred_json)\n",
+    "\n",
+    "coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
+    "coco_eval.evaluate()\n",
+    "coco_eval.accumulate()\n",
+    "coco_eval.summarize()\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "d78af9c4",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "id": "e6bc0593",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "loading annotations into memory...\n",
+      "Done (t=0.01s)\n",
+      "creating index...\n",
+      "index created!\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_18_14_171_0_-17_0_1_container_jpg.rf.3b7bb437394cb0b520cc27c78f74263f.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_10_41_945_1_17_0_1_container_jpg.rf.bf38b7cb2bcb1f3b0b87e236d4b6558f.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_18_14_171_1_-17_0_1_container_jpg.rf.d2028877c5e0d30cf6f7ffdcbec459af.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_10_41_945_0_17_0_1_container_jpg.rf.105fe7ec630616a5cf5fb0d77f7661ab.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_35_06_492_0_17_0_1_container_jpg.rf.feb8982c716709b5d18ac378fa19513a.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_07_10_208_0_17_0_1_container_jpg.rf.bd7a4b73e5f129945990a67fbd9c1a2e.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_24_44_832_0_17_0_1_container_jpg.rf.e35dca0b8420973aa558be59cd80b6e6.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_31_56_938_0_17_0_1_container_jpg.rf.b8787f893bbda1976cc864e97a5635dc.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_26_49_108_1_17_0_1_container_jpg.rf.68de61a268b2d1a26f7122dea64a43e0.jpg\n",
+      "Saved: /home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled/CAM_RT_01_2023_06_02_02_24_54_550_0_17_0_1_container_jpg.rf.18dc1aac582e88b12d37620a75f5a558.jpg\n"
+     ]
+    }
+   ],
+   "source": [
+    "import os\n",
+    "import json\n",
+    "from PIL import Image, ImageDraw\n",
+    "from pycocotools.coco import COCO\n",
+    "\n",
+    "# -----------------------------------------------------------------\n",
+    "out_dir     = '/home/computador/Desktop/models_research/sema_yolo_ASFF/mmyolo/mani-test/vis_relabeled'\n",
+    "os.makedirs(out_dir, exist_ok=True)\n",
+    "# -----------------------------------------------------------------\n",
+    "\n",
+    "# Load COCO ground truth and predictions\n",
+    "coco_gt = COCO(gt_ann_file)\n",
+    "with open(pred_json) as f:\n",
+    "    preds = json.load(f)\n",
+    "\n",
+    "# Build mapping from image_id -> predictions\n",
+    "pred_by_img = {}\n",
+    "for p in preds:\n",
+    "    pred_by_img.setdefault(p['image_id'], []).append(p)\n",
+    "\n",
+    "# Pick the first 10 images (sorted by ID for reproducibility)\n",
+    "first10 = sorted(coco_gt.getImgIds())[:10]\n",
+    "\n",
+    "for img_id in first10:\n",
+    "    img_info = coco_gt.loadImgs(img_id)[0]\n",
+    "    img_path = os.path.join(img_dir, os.path.basename(img_info['file_name']))\n",
+    "    if not os.path.exists(img_path):\n",
+    "        print(f\"Image not found: {img_path}\")\n",
+    "        continue\n",
+    "\n",
+    "    # Open image\n",
+    "    im = Image.open(img_path).convert(\"RGB\")\n",
+    "    draw = ImageDraw.Draw(im)\n",
+    "\n",
+    "    # ---- Draw ground truth boxes (green) ----\n",
+    "    ann_ids = coco_gt.getAnnIds(imgIds=img_id)\n",
+    "    anns = coco_gt.loadAnns(ann_ids)\n",
+    "    for ann in anns:\n",
+    "        x, y, w, h = ann['bbox']\n",
+    "        draw.rectangle([x, y, x + w, y + h], outline=\"green\", width=3)\n",
+    "        draw.text((x, y - 10), coco_gt.cats[ann['category_id']]['name'],\n",
+    "                  fill=\"green\")\n",
+    "\n",
+    "    # ---- Draw predicted boxes (red) ----\n",
+    "    for p in pred_by_img.get(img_id, []):\n",
+    "        x, y, w, h = p['bbox']\n",
+    "        score = p['score']\n",
+    "        draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=3)\n",
+    "        draw.text((x, y + h + 2),\n",
+    "                  f\"{p['category_id']}:{score:.2f}\",\n",
+    "                  fill=\"red\")\n",
+    "\n",
+    "    # Save image with overlays\n",
+    "    out_path = os.path.join(out_dir, os.path.basename(img_info['file_name']))\n",
+    "    im.save(out_path)\n",
+    "    print(f\"Saved: {out_path}\")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "37eea798",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "mmyolov2_asff",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.8.20"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}

diff --git a/mmyolo/models/backbones/ppyoloe_cspresnet_se_stage1.py b/mmyolo/models/backbones/ppyoloe_cspresnet_se_stage1.py
new file mode 100644
index 0000000..5364891
--- /dev/null
+++ b/mmyolo/models/backbones/ppyoloe_cspresnet_se_stage1.py
@@ -0,0 +1,165 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Tuple, Union
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from mmcv.cnn import ConvModule
+from mmdet.utils import ConfigType, OptMultiConfig
+from mmyolo.models.backbones import BaseBackbone
+from mmyolo.models.layers.yolo_bricks import CSPResLayer
+from mmyolo.registry import MODELS
+
+
+class SELayer(nn.Module):
+    """Lightweight Squeeze-and-Excitation (SE) block for shallow feature boosting."""
+
+    def __init__(self, channels: int, reduction: int = 16):
+        super().__init__()
+        mid = max(1, channels // reduction)
+        self.fc1 = nn.Conv2d(channels, mid, 1, bias=True)
+        self.relu = nn.ReLU(inplace=True)
+        self.fc2 = nn.Conv2d(mid, channels, 1, bias=True)
+        self.sigmoid = nn.Sigmoid()
+
+    def forward(self, x):
+        s = F.adaptive_avg_pool2d(x, 1)
+        s = self.fc1(s)
+        s = self.relu(s)
+        s = self.fc2(s)
+        s = self.sigmoid(s)
+        return x * s
+
+
+@MODELS.register_module()
+class PPYOLOECSPResNetSEStage1(BaseBackbone):
+    """PPYOLOE-CSPResNet backbone with SE attention inserted at Stage 1."""
+
+    arch_settings = {
+        'P5': [[64, 128, 3], [128, 256, 6], [256, 512, 6], [512, 1024, 3]],
+    }
+
+    def __init__(self,
+                 arch: str = 'P5',
+                 deepen_factor: float = 1.0,
+                 widen_factor: float = 1.0,
+                 input_channels: int = 3,
+                 out_indices: Tuple[int] = (2, 3, 4),
+                 frozen_stages: int = -1,
+                 plugins: Union[dict, List[dict]] = None,
+                 arch_ovewrite: dict = None,
+                 block_cfg: ConfigType = dict(
+                     type='PPYOLOEBasicBlock', shortcut=True, use_alpha=True),
+                 norm_cfg: ConfigType = dict(
+                     type='BN', momentum=0.1, eps=1e-5),
+                 act_cfg: ConfigType = dict(type='SiLU', inplace=True),
+                 attention_cfg: ConfigType = dict(
+                     type='EffectiveSELayer', act_cfg=dict(type='HSigmoid')),
+                 norm_eval: bool = False,
+                 init_cfg: OptMultiConfig = None,
+                 use_large_stem: bool = False,
+                 se_reduction: int = 16):
+        # Build base architecture settings
+        arch_setting = self.arch_settings[arch]
+        if arch_ovewrite:
+            arch_setting = arch_ovewrite
+        arch_setting = [[
+            int(in_channels * widen_factor),
+            int(out_channels * widen_factor),
+            round(num_blocks * deepen_factor)
+        ] for in_channels, out_channels, num_blocks in arch_setting]
+
+        self.block_cfg = block_cfg
+        self.use_large_stem = use_large_stem
+        self.attention_cfg = attention_cfg
+        self.se_reduction = se_reduction
+
+        super().__init__(
+            arch_setting,
+            deepen_factor,
+            widen_factor,
+            input_channels=input_channels,
+            out_indices=out_indices,
+            plugins=plugins,
+            frozen_stages=frozen_stages,
+            norm_cfg=norm_cfg,
+            act_cfg=act_cfg,
+            norm_eval=norm_eval,
+            init_cfg=init_cfg)
+
+        # Insert SE after Stage 1 output
+        out_channels_stage1 = arch_setting[0][1]
+        self.se_stage1 = SELayer(out_channels_stage1, reduction=se_reduction)
+
+    def build_stem_layer(self):
+        if self.use_large_stem:
+            stem = nn.Sequential(
+                ConvModule(
+                    self.input_channels,
+                    self.arch_setting[0][0] // 2,
+                    3,
+                    stride=2,
+                    padding=1,
+                    act_cfg=self.act_cfg,
+                    norm_cfg=self.norm_cfg),
+                ConvModule(
+                    self.arch_setting[0][0] // 2,
+                    self.arch_setting[0][0] // 2,
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg,
+                    act_cfg=self.act_cfg),
+                ConvModule(
+                    self.arch_setting[0][0] // 2,
+                    self.arch_setting[0][0],
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg,
+                    act_cfg=self.act_cfg))
+        else:
+            stem = nn.Sequential(
+                ConvModule(
+                    self.input_channels,
+                    self.arch_setting[0][0] // 2,
+                    3,
+                    stride=2,
+                    padding=1,
+                    norm_cfg=self.norm_cfg,
+                    act_cfg=self.act_cfg),
+                ConvModule(
+                    self.arch_setting[0][0] // 2,
+                    self.arch_setting[0][0],
+                    3,
+                    stride=1,
+                    padding=1,
+                    norm_cfg=self.norm_cfg,
+                    act_cfg=self.act_cfg))
+        return stem
+
+    def build_stage_layer(self, stage_idx: int, setting: list):
+        in_channels, out_channels, num_blocks = setting
+        cspres_layer = CSPResLayer(
+            in_channels=in_channels,
+            out_channels=out_channels,
+            num_block=num_blocks,
+            block_cfg=self.block_cfg,
+            stride=2,
+            norm_cfg=self.norm_cfg,
+            act_cfg=self.act_cfg,
+            attention_cfg=self.attention_cfg,
+            use_spp=False)
+        return [cspres_layer]
+
+    def forward(self, x):
+        outs = []
+        for i, layer_name in enumerate(self.layers):
+            layer = getattr(self, layer_name)
+            x = layer(x)
+            # Apply SE only after Stage 1 (index 1 in self.layers)
+            if layer_name == 'stage1':
+                x = self.se_stage1(x)
+            if i in self.out_indices:
+                outs.append(x)
+        return tuple(outs)
diff --git a/mmyolo/models/necks/base_yolo_neck.py b/mmyolo/models/necks/base_yolo_neck.py
index 8825b76..668dd1a 100644
--- a/mmyolo/models/necks/base_yolo_neck.py
+++ b/mmyolo/models/necks/base_yolo_neck.py
@@ -141,6 +141,7 @@ class BaseYOLONeck(BaseModule, metaclass=ABCMeta):
                  norm_cfg: ConfigType = None,
                  act_cfg: ConfigType = None,
                  init_cfg: OptMultiConfig = None,
+                 fusion_mode: str = None,   # ðŸ”¹ NEW ARG by @manikanta
                  **kwargs):
         super().__init__(init_cfg)
         self.in_channels = in_channels
@@ -152,6 +153,18 @@ class BaseYOLONeck(BaseModule, metaclass=ABCMeta):
         self.norm_cfg = norm_cfg
         self.act_cfg = act_cfg
 
+#--------------------------- by @manikanta ----------------------------------------------#
+        self.fusion_mode = fusion_mode # by @manikanta
+
+        # ðŸ”¹ Attach adaptive fusion module if enabled
+        if self.fusion_mode is not None:
+            from .ppyolo_neck_adaptive_fusion import attach_fusion_to_ppyolo_neck
+            attach_fusion_to_ppyolo_neck(self, mode=self.fusion_mode,
+                                            out_channels=self.out_channels[0] if isinstance(self.out_channels, list)
+                                            else self.out_channels)
+
+#-----------------------------------------------------------------------------------------#
+
         self.reduce_layers = nn.ModuleList()
         for idx in range(len(in_channels)):
             self.reduce_layers.append(self.build_reduce_layer(idx))
@@ -253,9 +266,21 @@ class BaseYOLONeck(BaseModule, metaclass=ABCMeta):
                 torch.cat([downsample_feat, feat_high], 1))
             outs.append(out)
 
+#------------------------- by @manikanta -------------------------------------#
         # out_layers
         results = []
         for idx in range(len(self.in_channels)):
             results.append(self.out_layers[idx](outs[idx]))
 
+        # ðŸ”¹ Apply fusion if available
+        if getattr(self, 'fusion_mode', None) is not None and hasattr(self, 'fusion'):
+            fused = self.fusion(results)
+            # ASFF returns list of same length, MFWF returns one tensor
+            if isinstance(fused, list):
+                results = fused
+            else:
+                results = [fused]
+
         return tuple(results)
+
+#------------------------------------------------------------------------------#
\ No newline at end of file
diff --git a/mmyolo/models/necks/ppyolo_neck_adaptive_fusion.py b/mmyolo/models/necks/ppyolo_neck_adaptive_fusion.py
new file mode 100644
index 0000000..4a2a62b
--- /dev/null
+++ b/mmyolo/models/necks/ppyolo_neck_adaptive_fusion.py
@@ -0,0 +1,275 @@
+"""
+Adaptive/Weighted feature fusion modules for PPYOLO in MMYOLO repository
+- Implements ASFF (adaptive spatial feature fusion) and MFWF (multi-scale feature weighted fusion)
+- Provides helper `attach_fusion_to_ppyolo_neck` showing how to integrate into PPYOLOECSPPAFPN
+
+How to use (summary):
+1. Put this file under mmyolo/models/necks/ (or project equivalent).
+2. Import and in PPYOLOECSPPAFPN __init__, add a `fusion_mode` argument and attach one of these modules:
+       self.fusion = ASFF(channels_list, target_level=..., reduction=8)  # or MFWF(...)
+   Then in forward, instead of `torch.cat(...)` use `self.fusion([feat_small, feat_mid, feat_large])`
+
+Notes:
+- This code aims to be drop-in friendly but you'll need to adapt exact channel orders and up/downsample helpers to your
+  project's conventions (some PPYOLO variants store P3,P4,P5 etc.).
+- Tests: run a quick forward pass with dummy tensors to ensure shapes match.
+
+References:
+- ASFF: https://arxiv.org/abs/1911.09516
+- MFWF concept: learn per-scale weights (simpler implementation here)
+
+"""
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class ConvBNAct(nn.Module):
+    def __init__(self, in_ch, out_ch, k=1, s=1, p=0, act=True):
+        super().__init__()
+        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False)
+        self.bn = nn.BatchNorm2d(out_ch)
+        self.act = nn.SiLU() if act else nn.Identity()
+
+    def forward(self, x):
+        return self.act(self.bn(self.conv(x)))
+
+
+class ASFFLevel(nn.Module):
+    """ASFF module for one target level. Accepts list of features [p3, p4, p5] (or similar)
+    and fuses them adaptively.
+
+    This is a pragmatic implementation (not an exact reproduction) designed to be easy to plug in.
+    """
+
+    def __init__(self, in_channels_list, out_channels, target_level=1, reduction=8):
+        """
+        in_channels_list: list of ints, channels for each scale (ordered from highest res to lowest res)
+        out_channels: channels for fused output
+        target_level: index of the target level to output (0..N-1). ASFF creates weights for all inputs
+        reduction: internal reduction for weight computation
+        """
+        super().__init__()
+        assert len(in_channels_list) >= 2, "need at least two scales"
+        self.num_scales = len(in_channels_list)
+        self.target_level = target_level
+
+        # Align convs: transform each input to the target spatial size and to a common channel dimension
+        self.resizers = nn.ModuleList()
+        for in_ch in in_channels_list:
+            # use 1x1 conv to reduce channels and then resize in forward
+            self.resizers.append(ConvBNAct(in_ch, out_channels, k=1, s=1, p=0))
+
+        # weight generation: for each scale produce a single-channel weight map
+        # We'll compute shared feature then per-scale conv to weight
+        self.weight_compress = ConvBNAct(out_channels * self.num_scales, out_channels // reduction, k=1, s=1, p=0)
+        self.weight_predictors = nn.ModuleList([
+            nn.Conv2d(out_channels // reduction, 1, kernel_size=1)
+            for _ in range(self.num_scales)
+        ])
+
+        # final conv to mix
+        self.output_conv = ConvBNAct(out_channels, out_channels, k=3, s=1, p=1)
+
+    def forward(self, feats):
+        # feats: list of tensors [f0 (high res), f1, f2 (low res)]
+        # target spatial size is that of feats[target_level]
+        target_size = feats[self.target_level].shape[-2:]
+
+        resized = []
+        for i, f in enumerate(feats):
+            x = self.resizers[i](f)  # channel align
+            if x.shape[-2:] != target_size:
+                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)
+            resized.append(x)
+
+        # channel-wise concat for weight computation
+        cat = torch.cat(resized, dim=1)
+        shared = self.weight_compress(cat)
+
+        weight_maps = [pred(shared) for pred in self.weight_predictors]  # list of [B,1,H,W]
+        weight_stack = torch.cat(weight_maps, dim=1)  # [B, S, H, W]
+        weight_soft = F.softmax(weight_stack, dim=1)  # softmax across scales
+
+        # weighted sum
+        fused = 0
+        for i in range(self.num_scales):
+            w = weight_soft[:, i:i+1, ...]
+            fused = fused + resized[i] * w
+
+        out = self.output_conv(fused)
+        return out
+
+
+class ASFF(nn.Module):
+    """ASFF wrapper for multiple target levels.
+    If you only need fusion to a single level (e.g., keep highest-level context), use a single ASFFLevel.
+    """
+
+    def __init__(self, in_channels_list, out_channels, reduction=8):
+        super().__init__()
+        self.levels = nn.ModuleList()
+        num = len(in_channels_list)
+        for target in range(num):
+            self.levels.append(ASFFLevel(in_channels_list, out_channels, target_level=target, reduction=reduction))
+
+    def forward(self, feats):
+        # returns list of fused features at each level, in same order as input
+        outs = []
+        for lv in self.levels:
+            outs.append(lv(feats))
+        return outs
+
+
+class MFWF(nn.Module):
+    """Multi-scale Feature Weighted Fusion (simpler, channel-wise learned weights)
+
+    Strategy:
+      - For each input scale, compute a per-channel scale weight using global pooling and small FC
+      - Normalize weights across scales (softmax) and produce weighted sum (after resizing spatially)
+    """
+
+    def __init__(self, in_channels_list, out_channels=None, reduction=16):
+        super().__init__()
+        self.num_scales = len(in_channels_list)
+        # Use out_channels if specified else keep channels of target (assume first)
+        if out_channels is None:
+            out_channels = in_channels_list[0]
+        self.out_channels = out_channels
+
+        # Align channels
+        self.align_convs = nn.ModuleList([ConvBNAct(ch, out_channels, k=1, s=1, p=0) for ch in in_channels_list])
+
+        # per-scale channel weight generators
+        self.fc_generators = nn.ModuleList()
+        for _ in in_channels_list:
+            self.fc_generators.append(
+                nn.Sequential(
+                    nn.AdaptiveAvgPool2d(1),
+                    nn.Conv2d(out_channels, out_channels // reduction, kernel_size=1, bias=False),
+                    nn.ReLU(inplace=True),
+                    nn.Conv2d(out_channels // reduction, out_channels, kernel_size=1, bias=False)
+                )
+            )
+
+        self.final_conv = ConvBNAct(out_channels, out_channels, k=3, s=1, p=1)
+
+    def forward(self, feats):
+        # feats: list of tensors
+        target_size = feats[0].shape[-2:]  # choose highest-res as target by default
+
+        aligned = []
+        for i, f in enumerate(feats):
+            x = self.align_convs[i](f)
+            if x.shape[-2:] != target_size:
+                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)
+            aligned.append(x)
+
+        # compute channel logits per scale
+        logits = []
+        for i, x in enumerate(aligned):
+            l = self.fc_generators[i](x)  # [B, C, 1, 1]
+            logits.append(l)
+
+        logits_stack = torch.stack(logits, dim=1)  # [B, S, C, 1, 1]
+        # normalize across scales for each channel -> softmax on dim=1
+        logits_stack = logits_stack.squeeze(-1).squeeze(-1)  # [B, S, C]
+        weight = F.softmax(logits_stack, dim=1)  # [B, S, C]
+
+        # apply weights and sum
+        fused = 0
+        for i in range(self.num_scales):
+            w = weight[:, i:i+1, :].unsqueeze(-1).unsqueeze(-1)  # [B,1,C,1,1]
+            fused = fused + aligned[i] * w
+
+        # fused shape [B, C, H, W]
+        out = self.final_conv(fused)
+        return out
+
+
+# Integration helper
+
+def attach_fusion_to_ppyolo_neck(neck_module, mode='asff', out_channels=None, reduction=8):
+    """
+    neck_module: instance of PPYOLOECSPPAFPN (or similar) -- not the class
+    mode: 'asff' or 'mfwf'
+    out_channels: desired output channel size for fused features. If None, inferred from neck
+
+    This function will attach `neck_module.fusion` and provide guidance for modifying forward.
+
+    Example modification in PPYOLOECSPPAFPN.__init__:
+        self.fusion_mode = fusion_mode  # e.g. 'asff'
+        self.fusion = None
+        if fusion_mode is not None:
+            from .ppyolo_neck_adaptive_fusion import attach_fusion_to_ppyolo_neck
+            attach_fusion_to_ppyolo_neck(self, mode=fusion_mode, out_channels=some_ch)
+
+    And in forward, replace concatenation logic (where features are fused) with:
+        if self.fusion is not None:
+            fused = self.fusion([feat_high_res, feat_mid, feat_low])  # depending on ordering
+            # if ASFF returns list (for each level), adapt accordingly
+
+    NOTE: Adjust the order of features passed according to your neck's indexing.
+    """
+    # try to detect channel sizes from neck attributes (common names)
+    chs = []
+    candidates = [
+        getattr(neck_module, 'in_channels', None),
+        getattr(neck_module, 'out_channels', None),
+    ]
+    # fallback: try to inspect some feature extraction conv layers
+    # This detection is best-effort; you may want to pass channels explicitly.
+    if out_channels is None:
+        # attempt to infer channels from neck convs if available
+        for name, m in neck_module.named_modules():
+            if isinstance(m, nn.Conv2d) and m.kernel_size == (1, 1):
+                chs.append(m.out_channels)
+                if len(chs) >= 3:
+                    break
+        if chs:
+            out_channels = chs[0]
+        else:
+            raise RuntimeError('Could not infer out_channels for fusion; please pass out_channels explicitly')
+
+    # For typical PPYOLO neck we fuse three scales: [P3, P4, P5]
+    # Expect channel sizes are equal; if not, user should pass explicit in_channels_list
+    # Provide a conservative default: use neck's out_channels if exists
+    if hasattr(neck_module, 'out_channels') and neck_module.out_channels is not None:
+        default_ch = neck_module.out_channels
+    else:
+        default_ch = out_channels
+
+    # create in_channels_list for 3 scales
+    in_channels_list = [default_ch] * 3
+
+    if mode.lower() == 'asff':
+        neck_module.fusion = ASFF(in_channels_list, out_channels=out_channels, reduction=reduction)
+    elif mode.lower() == 'mfwf' or mode.lower() == 'mfwf':
+        neck_module.fusion = MFWF(in_channels_list, out_channels=out_channels, reduction=reduction)
+    else:
+        raise ValueError('Unknown fusion mode: ' + str(mode))
+
+    # attach meta info so forward patch is easier
+    neck_module._fusion_mode = mode.lower()
+    neck_module._fusion_channels = out_channels
+
+    return neck_module
+
+
+# Quick sanity test (run standalone)
+if __name__ == '__main__':
+    # create dummy features: P3 (80x80), P4 (40x40), P5 (20x20) for input image 640
+    b = 2
+    C = 256
+    p3 = torch.rand(b, C, 80, 80)
+    p4 = torch.rand(b, C, 40, 40)
+    p5 = torch.rand(b, C, 20, 20)
+
+    asff = ASFF([C, C, C], out_channels=C)
+    outs = asff([p3, p4, p5])
+    print('ASFF outputs:', [o.shape for o in outs])
+
+    mf = MFWF([C, C, C], out_channels=C)
+    out_m = mf([p3, p4, p5])
+    print('MFWF output:', out_m.shape)
diff --git a/mmyolo/models/necks/ppyoloe_csppan.py b/mmyolo/models/necks/ppyoloe_csppan.py
index 4e4ef72..4e5b952 100644
--- a/mmyolo/models/necks/ppyoloe_csppan.py
+++ b/mmyolo/models/necks/ppyoloe_csppan.py
@@ -60,7 +60,9 @@ class PPYOLOECSPPAFPN(BaseYOLONeck):
                  act_cfg: ConfigType = dict(type='SiLU', inplace=True),
                  drop_block_cfg: ConfigType = None,
                  init_cfg: OptMultiConfig = None,
-                 use_spp: bool = False):
+                 fusion_mode: str = None,   # ðŸ”¹ NEW ARG by @manikanta
+                 use_spp: bool = False
+                 ):
         self.block_cfg = block_cfg
         self.num_csplayer = num_csplayer
         self.num_blocks_per_layer = round(num_blocks_per_layer * deepen_factor)
@@ -82,7 +84,9 @@ class PPYOLOECSPPAFPN(BaseYOLONeck):
             norm_cfg=norm_cfg,
             act_cfg=act_cfg,
             init_cfg=init_cfg)
+        self.fusion_mode = fusion_mode # by @ manikanta
 
+        
     def build_reduce_layer(self, idx: int):
         """build reduce layer.
 
